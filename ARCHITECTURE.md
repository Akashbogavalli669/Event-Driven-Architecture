# System Architecture

This document provides a high-level overview of the Event-Driven Order Processing System. It details the data flow, component interactions, and the design decisions made to ensure reliability and idempotency.

## 1. High-Level Overview

The system is designed to decouple the **ingestion** of orders from the **processing** of orders. This separation of concerns allows the Producer API to handle high-throughput traffic with low latency (`202 Accepted`), while the Consumer Service processes complex business logic asynchronously at its own pace.

### Core Components

* **Producer Service (FastAPI):** Acts as the entry point. It validates incoming JSON payloads against a strict schema (Pydantic) and publishes messages to the Kafka topic `order_events`.
* **Apache Kafka:** The distributed event streaming platform that acts as the durable buffer between the producer and consumer.
* **Consumer Service (Python Worker):** Subscribes to `order_events`. It is responsible for business logic and persisting the final state.
* **PostgreSQL:** The relational database used for persistent storage. It plays a critical role in the idempotency strategy via Unique Constraints.

## 2. Event Processing Flow (Sequence Diagram)

The following diagram illustrates the lifecycle of a single order event, including the "Happy Path" and the handling of duplicate messages.

```mermaid
sequenceDiagram
    participant Client
    participant API as Producer API
    participant Kafka as Kafka (Topic: order_events)
    participant Consumer as Consumer Worker
    participant DB as PostgreSQL

    %% Happy Path
    Note over Client, DB: Scenario 1: Standard Processing (Happy Path)
    Client->>API: POST /events (Order A)
    API->>API: Validate Schema
    API->>Kafka: Publish (EventID: 123)
    Kafka-->>API: Ack
    API-->>Client: 202 Accepted

    Kafka->>Consumer: Push Event (EventID: 123)
    Consumer->>DB: INSERT INTO processed_events (id: 123...)
    DB-->>Consumer: Success
    Consumer->>Kafka: Commit Offset
    Note over Consumer: Log: "Event Processed Successfully"

    %% Idempotency Scenario
    Note over Client, DB: Scenario 2: Duplicate Event (Idempotency)
    Kafka->>Consumer: Re-deliver Event (EventID: 123)
    Consumer->>DB: INSERT INTO processed_events (id: 123...)
    DB-->>Consumer: ERROR: Unique Constraint Violation
    Consumer->>Consumer: Catch IntegrityError
    Note over Consumer: Log: "Duplicate detected. Skipping."
    Consumer->>Kafka: Commit Offset (Mark as done)
    ```

    ## 3. Idempotency Strategy

One of the core requirements of this system is **Exactly-Once Processing (effectively)**.

Since **Kafka guarantees At-Least-Once delivery** (messages may be delivered more than once due to retries), the **consumer must be idempotent**.

### Database-Level Idempotency (Implemented)

We implemented **idempotency at the database layer**, leveraging PostgreSQL’s transactional guarantees.

#### Key Concepts

- **Unique Identifier**
  - Every event includes a `event_id` (UUID)
  - Generated by the producer or client

- **Atomic Verification**
  - A `processed_events` table enforces a **PRIMARY KEY / UNIQUE constraint** on `event_id`
  - Relies on PostgreSQL **ACID properties**

#### Processing Logic

1. The consumer attempts to **INSERT** the `event_id` into `processed_events`
2. **If insert succeeds**
   - The event is new
   - Business logic is executed
3. **If insert fails (IntegrityError)**
   - The event was already processed
   - Error is logged
   - Kafka offset is committed
   - Consumer safely moves on

#### Why This Approach?

This strategy is more robust than a **Check-Then-Act** pattern (`SELECT` → `INSERT`), which is vulnerable to race conditions in high-concurrency systems.

Using a **single atomic INSERT** guarantees correctness even under concurrent consumers.

---

## 4. Architectural Trade-offs & Decisions

| Decision Area | Alternative Considered | Rationale |
|---------------|------------------------|-----------|
| **Database** | MongoDB (NoSQL) | **Consistency over flexibility**. PostgreSQL provides strong ACID guarantees and native unique constraints, making strict idempotency simpler and safer than eventual consistency models. |
| **Validation** | Manual Regex / Custom Checks | **Safety & correctness**. Pydantic offers declarative schema validation, preventing malformed or “poison pill” messages from reaching Kafka consumers. |
| **Orchestration** | Kubernetes / Local Shell Scripts | **Simplicity**. Docker Compose allows the entire stack (Zookeeper, Kafka, DB, Services) to start with a single command, avoiding environment drift. |
| **Language** | Go / Java | **Speed of development**. Python async libraries (`aiokafka`, `asyncpg`) provide high-performance non-blocking I/O while keeping the codebase readable and review-friendly. |

---

